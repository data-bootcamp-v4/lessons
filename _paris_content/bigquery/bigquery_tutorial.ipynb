{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle large data sets with BigQuery \n",
    "Tutorial created by [Thomas Belhalfaoui](https://www.linkedin.com/in/belhalfaoui/?originalSubdomain=fr) for [Ironhack](https://www.ironhack.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why and when to use big data engines?\n",
    "\n",
    "Before we start, let's talk a bit about what big data engines are, why end when they are useful, and what are their upsides and downsides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Relational databases and vertical scalability\n",
    "\n",
    "When you have a dataset, you can just store it in a relational database (e.g. MySQL). If the dataset is larger, the solution is easy: just buy a bigger machine for your database (more RAM and larger hard drive)! This is what we call **vertical scalability**.\n",
    "\n",
    "It works, but having to change the machine every time the dataset gets too big is not very handy.\n",
    "\n",
    "Plus, but what happens when you reach dozens of terabytes of data? Most probably, you will not find any computer that has such a large hard drive, not even speaking of the RAM. And even for smaller datasets (a couple of terabytes), it may happen that some features of the MySQL database (mostly joins) will become almost unusable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Big data engines, sharding and horizontal scalability\n",
    "\n",
    "To solve this issues, there is an idea: why not **split** the data across multiple machines? This exists: it is called **sharding** (each **split** is a **shard**) and it is part of what we call **horizontal scalability**. This is what **big data storage and query engine** do.\n",
    "\n",
    "But now that the data is spread across several machines, how do we know where each data lies? This is why the **sharding key** is for. For instance, if the sharding key can be a `year` column, then all rows with the same value of `year` will go the the same shard (machine). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Big data engines and denormalization: loss of flexibility and data redundancy\n",
    "\n",
    "So big data engines are perfect? Of course not. There is one big drawback: **we cannot (easily) do _joins_**.\n",
    "\n",
    "Indeed, when two tables are sharded, making a _join_ between the two of them means moving a lot of data across the Internet. Imagine the data is sharded by `year` but you join on the `client_id` column. Most probably, for a given client ID, the rows of table 1 do not lie on the same machine as the rows of table 2. So one of the two has to travel on the Internet to go to the same machine for later aggregation.\n",
    "\n",
    "And network transfer is _very_ slow (several orders of magnitude slower than reading from a hard drive). What is worse, the different shards do not have to be physically close - actually they can even be quite far away, sometimes in different countries.\n",
    "\n",
    "So what do you do when you cannot do joins: you **pre-join**. This is called **denormalizing** the data. Instead of having multiple tables that relate to each other like in a relational (e.g. MySQL) database (aka a **normalized** format), we have one big table with many columns, with everything already joined together in some way.\n",
    "\n",
    "Of course, you understand what the biggest drawback of this approach is: you have to decide in advance how you want to join (denormalize) the dataset and you can do it only once! You can do it several times but this means you store several copies of the full denormalized dataset. So you **loose the flexibility** that you had with relational databases.\n",
    "\n",
    "And the second drawback is that it uses **much more storage space** since there is a lot of **data redundancy**. Imagine you have:\n",
    "* A `Product` table with a `productId` column and all product metadata (`productName`, etc.),\n",
    "* A `Sales` tables with `date` and `productId`.\n",
    "\n",
    "Now we denormalize the dataset (aka join on `productId`). So each time a product is sold, we don't just have the `productId` (a small integer) but we have ALL the product columns (`productName`, etc.) that are **duplicated for each sale**! In termes of storage space it is huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get familiar with BigQuery and its interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. First steps with the interface\n",
    "You can access the Google Cloud Platform (GCP) Console at: https://console.cloud.google.com/. Then click on _Big Query_ and choose project `da-bootcamp-2023` on the top left-hand side corner of the page.\n",
    "\n",
    "Or you can directly go to: https://console.cloud.google.com/bigquery?project=da-bootcamp-2023\n",
    "\n",
    "You will have to log into a Google account. Use your personal one: if everything went as planned, you should have already been added to the `da-bootcamp-2023` project with your personal Gmail address. Otherwise, please raise your hand!\n",
    "\n",
    "You should land up on a page similar to this one:\n",
    "\n",
    "<img src=\"bigquery_homepage.png\" width=\"800\">\n",
    "\n",
    "You may have noticed the `bigquery-public-data` line in the _Explorer_ block. It is a public _project_ published by Google, that contains multiple open datasets you can use. To make in also appear in your interface, click on _\"+ Add\"_ in the top bar, choose _\"Public datasets\"_ and click on one of them, for instance _\"About Covid-19 public datasets\"_ and then _\"VIEW DATASET\"_.\n",
    "\n",
    "Alternatively, you can directly follow this link: https://console.cloud.google.com/marketplace/product/bigquery-public-datasets/covid19-public-data-program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The hierarchy\n",
    "\n",
    "All the data in BigQuery is structured in a hierarchical way, that you can see in the _Explorer_ block:\n",
    "\n",
    "1. **Project** (here `bigquery-public-data`),\n",
    "\n",
    "2. **Dataset** (e.g. `covid19_nyt`),\n",
    "\n",
    "3. **Table** (e.g. `mask_use_by_county`).\n",
    "\n",
    "A table can be called by its full name: `project.dataset.table` (e.g. `bigquery-public-data.covid19_nyt.mask_use_by_county`).\n",
    "In case you already selected the `bigquery-public-data` project, though, then you can skip the first part and just call it `covid19_nyt.mask_use_by_county`.\n",
    "\n",
    "\n",
    "In this tutorial, we will mostly:\n",
    "* Read data from the public dataset `bigquery-public-data.covid19_open_data`,\n",
    "* Read and write data from and to our dataset `da-bootcamp-2023.myfirstname`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Create my dataset\n",
    "\n",
    "The first step for you is to create your personal dataset, where you will then create your tables.\n",
    "\n",
    "To do so, click on the three dots and click on _\"Create dataset\"_:\n",
    "\n",
    "<img src=\"bigquery_create_dataset.png\" width=\"400\">\n",
    "\n",
    "Type your first name as the name of the dataset (lower case, no space, no special character), leave everything else as is and confirm. You should now see your personal dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. The first GoogleSQL query!\n",
    "\n",
    "SQL is a specification: think of it as a family of query languages that are very similar. They share almost everything but each of them have some slight differences (more on this in section 3).\n",
    "\n",
    "For now, you can consider that what you know from MySQL also applies to GoogleSQL (which is how Google calls the BigQuery flavor of SQL).\n",
    "\n",
    "Let's make this query to test that everything works. It is silly and useless but free... (we will see later on that each action we make on BigQuery involves some charges). Maybe you don't know it, but you can make a `SELECT` without any table to return just a constant value:\n",
    "```sql\n",
    "SELECT 'hello' AS message\n",
    "```\n",
    "\n",
    "From the BigQuery homepage, click on _\"COMPOSE A NEW QUERY\"_ or the _\"+\"_ in the tab bar:\n",
    "\n",
    "<img src=\"new_query.png\" width=\"270\">\n",
    "\n",
    "Then, in the query editor, type your query and click on _\"RUN\"_ (you can also hit CTRL+Enter).\n",
    "\n",
    "<img src=\"query_editor.png\" width=\"800\">\n",
    "\n",
    "Several remarks about this screen:\n",
    "* In the bottom part of the screen, you can see (and export) the results of your query.\n",
    "\n",
    "* You can also have _\"EXECUTION DETAILS\"_, that gives you some measures about your job (time, consumption, etc.).\n",
    "* Also, _\"EXECUTION GRAPH\"_ gives you interesting details about the interals of the query engine: it tells you the exact steps that have been done to get your query.\n",
    "\n",
    "* In the top right corner, there is this message: `This query will process 0 B when run.` This gives you the cost of your query (very important!). More on this in section 3.\n",
    "\n",
    "* On the top bar, you can notice the \"SAVE\" button: you can (should) use it!\n",
    "\n",
    "For now, there is not much execution details nor execution graph, because the query is too simple. But there will be very soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How to optimize time and cost despite columnar storage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. LIMIT is useless\n",
    "\n",
    "Let's try this query (**don't run it, just write it in the editor!**)\n",
    "```sql\n",
    "SELECT * FROM `bigquery-public-data.covid19_open_data.covid19_open_data`;\n",
    "```\n",
    "\n",
    "You should see in the top right corner a green check that says:\n",
    "> This query will process 11.71 GB when run.\n",
    "By the way, if instead of the green check you get a red cross, it will tell you the error in your query and where it comes from.\n",
    "\n",
    "So let's change the query to save some money, because say we only need the first 100 rows to see what the data looks like:\n",
    "```sql\n",
    "SELECT * FROM `bigquery-public-data.covid19_open_data.covid19_open_data` LIMIT 100;\n",
    "```\n",
    "\n",
    "Easy, right?\n",
    "\n",
    "Oops! The cost is the same...\n",
    "> This query will process 11.71 GB when run.\n",
    "\n",
    "Let's look at the table information (just open the `covid19_open_data` dataset and then on the `covid19_open_data` table inside).\n",
    "\n",
    "<img src=\"table_info.png\" width=\"600\">\n",
    "\n",
    "Actually, 11.71 GB corresponds to the whole table! But why read the whole table since we need only the first 100 rows.\n",
    "\n",
    "The answer is simple: because BigQuery **stores data in columns** (it is a **columnar storage**) and not in rows.\n",
    "\n",
    "So if you need only the first 100 rows of each column, it still has to read all the columns. And once a column is read, it does not matter how many rows we need: it takes roughly the same time (and exactly the same cost) to read 1 value or 10 million values.\n",
    "\n",
    "---\n",
    "If there is a single thing you should remember from BigQuery it is this:\n",
    "* Data is stored in columns.\n",
    "* Each column accessed is entirely billed (except for partitions, but we will see that later).\n",
    "* Billing is per data read and written (not per data returned).\n",
    "* Always look at the cost in the top right corner before running a query.\n",
    "---\n",
    "\n",
    "So what can we do to pay (and wait) less?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Think before doing\n",
    "\n",
    "> This is the first lesson in the big data world: do less and think more before running a query.\n",
    "\n",
    "A query can be costly and time consuming so we want to make sure it works, it is useful and it gives the right results before running it.\n",
    "\n",
    "There are two (free) very interesting tools that we highly recommend you use extensively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. Data schema\n",
    "\n",
    "You can get the list of all columns along with their type.\n",
    "\n",
    "<img src=\"table_schema.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Data preview\n",
    "\n",
    "> **This it THE tool you must use. It is free. Use it instead of `SELECT * FROM ... LIMIT ...`.**\n",
    "\n",
    "<img src=\"table_preview.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. SELECT fewer columns\n",
    "\n",
    "Now that we have looked at the preview and we know what the data looks like, let's ask ourselves: do I really need all the columns?\n",
    "\n",
    "Yes, because since BigQuery is a **columnar storage**, each column that we remove will reduce the cost and time by **a lot**!\n",
    "\n",
    "Remember that in the big data world, datasets are **denormalized**. It means that all the columns anyone _might_ once need are all there, pre-joined for you. But it means there are *a lot* of columns.\n",
    "\n",
    "We didn't find an easy way to get the total number of columns in a table (if you find one, we are interested!). But we still can get the information by querying the special metadata table called `INFORMATION_SCHEMA` (there is one per dataset):\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    COUNT(distinct column_name) \n",
    "FROM `bigquery-public-data.covid19_open_data.INFORMATION_SCHEMA.COLUMNS`\n",
    "WHERE table_name = \"covid19_open_data\";\n",
    "```\n",
    "\n",
    "Anyway, the fact is: the dataset has 701 columns!! For sure, we don't need them all.\n",
    "\n",
    "Let's try this one:\n",
    "```sql\n",
    "SELECT\n",
    "    date,\n",
    "    country_name,\n",
    "    new_confirmed\n",
    "FROM `bigquery-public-data.covid19_open_data.covid19_open_data`;\n",
    "```\n",
    "\n",
    "Boom! Only **~500 Mb** of data processed: divided cost and time by almost 20!\n",
    "\n",
    "Now we can run the query :)\n",
    "\n",
    "In the bottom part of the window, you can see the results as a grid. You can also look at execution details to see how much time it took and how much data has been processed (and other details).\n",
    "\n",
    "Also useful: on the very bottom bar, you have _\"PERSONAL HISTORY\"_ where you can access all the previous queries you made (even if you forgot to save them in the editor..)\n",
    "\n",
    "<img src=\"execution_details_and_history.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Partitioning and clustering - how to save time and money?\n",
    "\n",
    "This is quite annoying that we must query the whole column each time we need only part of it... Could we do something about it?\n",
    "\n",
    "Well, there is no magical solution but it turns out that yes, we can do something: it is called *partitioning* and *clustering*.\n",
    "\n",
    "First let us say that the word _clustering_ is really a poor choice here, since it is more some king of _sorting_ - but so it is. So what is it about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Partitioning\n",
    "\n",
    "The broad idea is to split all the columns in _chunks_ (**partitions**) according to the value of one of them.\n",
    "\n",
    "Say we often access the COVID-19 data by date (i.e. with a `WHERE date ...` filter). Then it would be clever to:\n",
    "1. Split the table into chunks, for instance one chunk per month (or multiple months in one chunk).\n",
    "2. Write in some easily-accessible (e.g. in-memory) index the mapping between the month and the memory address where we stored this chunk.\n",
    "\n",
    "This way, when we do a query like `SELECT ... WHERE date = \"2021-05-15`, BigQuery can use the index to read (and charge for) **only** the **2021-05** chunk (which contains all columns for which the date is in May 2021).\n",
    "\n",
    "Perfect, isn't it? Well... there is no free lunch.\n",
    "\n",
    "* The biggest drawback: you can only **partition by one column** in a table. So you have to think hard in advance and choose the right one!\n",
    "\n",
    "* It is costly to change the partitioning column (aka _repartition_).\n",
    "\n",
    "* You can have **at most 4 000 partitions per table**.\n",
    "\n",
    "* The partitioning index takes some additional space to store.\n",
    "\n",
    "NB: All this means that it is a bad idea to partition on a column that has a lot of different value (not even speaking of a unique column like an ID - which would be a _very_ bad idea).\n",
    "\n",
    "For more information about partitioning: https://cloud.google.com/bigquery/docs/partitioned-tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Clustering\n",
    "\n",
    "Again the word _clustering_ is inadequate (it is more _sorting_). So what does _clustering_ do?\n",
    "\n",
    "First of all, as opposed to partitioning, clustering can be set on a list of columns (in a fixed order).\n",
    "\n",
    "What BigQuery does it **sort the rows of the table**:\n",
    "* By the first column,\n",
    "* Then (if the values in the first column are equal) by the second column,\n",
    "* Etc.\n",
    "\n",
    "Why is it interesting? Well because there are clever lookup algorithms that work well when data is sorted (like for instance dictotomic search: https://en.wikipedia.org/wiki/Dichotomic_search).\n",
    "\n",
    "It is not as good as partitioning but still:\n",
    "\n",
    "* Within a single (large) partition, it helps reduce the amount of data processed.\n",
    "\n",
    "* If you want to filter by the columns you clustered by (but not partitioned by) then clustering helps (not as good as if you partitioned by the filter you use, but better than nothing or than repartitioning the whole table)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Partitioning and clustering together in practice\n",
    "\n",
    "If you look into the _\"DETAILS\"_ page of the `covid19_open_data` table you will see that it is not partionned nor clustered.\n",
    "\n",
    "So we will create a new table with:\n",
    "* Only the columns we need,\n",
    "* Some nice partitioning and clustering.\n",
    "\n",
    "```sql\n",
    "CREATE TABLE `myfirstname.covid19_first`\n",
    "PARTITION BY date\n",
    "CLUSTER BY date,country_name\n",
    "AS\n",
    "SELECT\n",
    "  date,\n",
    "  country_name,\n",
    "  new_tested,\n",
    "  new_confirmed,\n",
    "  new_hospitalized_patients,\n",
    "  new_deceased\n",
    "FROM `bigquery-public-data.covid19_open_data.covid19_open_data`;\n",
    "```\n",
    "\n",
    "A couple of remarks:\n",
    "\n",
    "* We can only partition on a `DATE` or numerical column (`INT64`, `FLOAT`, etc.). If you need to partition on a `STRING` column, you can use the very handy function `FARM_FINGERPRINT` that takes a string and returns an integer (more on this here: https://cloud.google.com/bigquery/docs/reference/standard-sql/hash_functions#farm_fingerprint).\n",
    "\n",
    "* The number of partitions is determined automatically by BigQuery.\n",
    "\n",
    "* It takes some time for the nuber of partitions to be updated (maybe a couple of minutes). You can go to the _\"DETAILS\"_ section of the new table and click _\"REFRESH\"_ until the partition number is not zero.\n",
    "\n",
    "* Different dates can end up in the same partition (if there are more distinct dates than possible partitions).\n",
    "\n",
    "* Why cluster by `date`, since we already partitioned by `date`? Well because of the revious remark. If you have multiple dates in the same partition, you may still want to optimize the query time _within_ the partition.\n",
    "\n",
    "Here you see illutrated in practice the fact we talked about in the introduction, that with big data query engines you loose flexibility. You must plan in advance (when you create the table) for the type of queries you will do in the future..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Query a partitioned and clustered table\n",
    "\n",
    "To test if you undestood these concepts, try to guess the amount of data processed by each of the following queries (not in absolute terms of course, but guess which one is more costly than which one):\n",
    "\n",
    "```sql\n",
    "-- Query A: no filter\n",
    "SELECT * FROM `myfirstname.covid19_first`;\n",
    "\n",
    "-- Query B: filter on one day\n",
    "SELECT * FROM `myfirstname.covid19_first`\n",
    "WHERE date = \"2021-05-15\";\n",
    "\n",
    "-- Query C: filter on one month\n",
    "SELECT * FROM `myfirstname.covid19_first`\n",
    "WHERE date BETWEEN \"2021-05-01\" AND \"2021-05-31\";\n",
    "\n",
    "-- Query D: filter on one country\n",
    "SELECT * FROM `myfirstname.covid19_first`\n",
    "WHERE country_name = 'France';\n",
    "\n",
    "-- Query E: filter on one day and one country\n",
    "SELECT * FROM `myfirstname.covid19_first`\n",
    "WHERE\n",
    "  date = \"2021-05-15\"\n",
    "  AND country_name = 'France';\n",
    "\n",
    "-- Query F: filter on one month and one country\n",
    "SELECT * FROM `myfirstname.covid19_first`\n",
    "WHERE\n",
    "  date BETWEEN \"2021-05-01\" AND \"2021-05-31\"\n",
    "  AND country_name = 'France';\n",
    "\n",
    "```\n",
    "\n",
    "And the answer is...\n",
    "\n",
    "| Query                                    | Amount of data processed |\n",
    "| ------------------------------------     | ------------------------ |\n",
    "| A: no filter                             | 619.27 MB                |\n",
    "| D: filter on one country                 | 619.27 MB                |\n",
    "| C: filter on one month                   | 21.47 MB                 |\n",
    "| F: filter on one month and one country   | 21.47 MB                 |\n",
    "| B: filter on one day                     | 702.14 KB                |\n",
    "| E: filter on one day and one country     | 802.14 KB                |\n",
    "\n",
    "What do we see?\n",
    "\n",
    "Partitioning works as expected:\n",
    "\n",
    "* One month query processes 31 times more data than one day query.\n",
    "\n",
    "* One month query takes ~30 times less time than the full dataset query (there is around 3 years of data in the table).\n",
    "\n",
    "* Filtering by country has no particular effect, since the table is not partiioned by country.\n",
    "\n",
    "\n",
    "But what is weird, is that clustering seems to have no effect at all... There are two reasons to that:\n",
    "\n",
    "* **BigQuery cannot estimate in advance the cost reduction due to clustering**. So the estimate it gives you is a _worst case scenario_. You can only get the _actual_ amount of data processed if you _actually_ run the query.\n",
    "\n",
    "* But in this case, even if we run the queries, we don't see a difference - for a tricky reason.<br/>\n",
    "Remember clustering sorts by `date` and then **if (and only if!) the day is the same**, then it sorts by `country_name`. But the amount of data for one day is tiny (~800 KB). So the sort by `country_name` **within one day** is useless (800 KB is probably below the size of the smallest data chunk stored by BigQuery anyway).\n",
    "\n",
    "So it is a bad idea to cluster by a column with has many distinct values - especially if it is the first clustering column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. See the effect of clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.1. First attempt\n",
    "A better choice seems then to be to partition by `country_name` only.\n",
    "\n",
    "```sql\n",
    "CREATE TABLE `myfirstname.covid19_second`\n",
    "PARTITION BY date\n",
    "CLUSTER BY country_name\n",
    "AS\n",
    "SELECT\n",
    "  date,\n",
    "  country_name,\n",
    "  new_tested,\n",
    "  new_confirmed,\n",
    "  new_hospitalized_patients,\n",
    "  new_deceased\n",
    "FROM `bigquery-public-data.covid19_open_data.covid19_open_data`;\n",
    "```\n",
    "\n",
    "Now let's check:\n",
    "```sql\n",
    "SELECT * FROM `myfirstname.covid19_second`\n",
    "WHERE country_name = 'France';\n",
    "```\n",
    "\n",
    "<img src=\"data_processed_bis.png\" width=\"450\">\n",
    "\n",
    "Still no luck! It processes the whole dataset even when we ask for one country. So no effect of clustering at all...\n",
    "\n",
    "What happened? Well now the issue is the interaction between clustering and partitioning. Indeed, remember that **clustering occurs only within each partition**. So even if we just cluster by `country_name` (which should work), the partitioning by `date` kills the potential effect of clustering. The partitions (by day) are so small that clustering has no effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2. Second attempt\n",
    "So what we need to do is to increase the size of the partitions. Instead of partitioning by day we will partition by year.\n",
    "\n",
    "To do so, we will use the hendy function `DATE_TRUNC` which truncates the date (here to the year). For instance, a date like `2021-05-15` will be transformed into `2021-01-01` (like all other days of 2021). So all these 2021 days will end up in the same partition.\n",
    "\n",
    "```sql\n",
    "CREATE TABLE `myfirstname.covid19_third`\n",
    "PARTITION BY DATE_TRUNC(date, YEAR)\n",
    "CLUSTER BY country_name\n",
    "AS\n",
    "SELECT\n",
    "  date,\n",
    "  country_name,\n",
    "  new_tested,\n",
    "  new_confirmed,\n",
    "  new_hospitalized_patients,\n",
    "  new_deceased\n",
    "FROM `bigquery-public-data.covid19_open_data.covid19_open_data`;\n",
    "```\n",
    "\n",
    "Now let's check:\n",
    "```sql\n",
    "SELECT * FROM `myfirstname.covid19_third`\n",
    "WHERE country_name = 'France';\n",
    "```\n",
    "\n",
    "<img src=\"data_processed_ter.png\" width=\"400\">\n",
    "\n",
    "Hurrah! This time it worked. The amount of data processed is only 40 MB, which is much smaller than the 619 MB of the whole table.\n",
    "\n",
    "NB: The _estimated_ amount displayed in the top right corner is still overestimated (619 MB), because BigQuery cannot know in advance how much it will save thanks to clustering.\n",
    "\n",
    "So to recap on clustering:\n",
    "\n",
    "---\n",
    "* Don't cluster on a column that has too many distinct values.\n",
    "\n",
    "* The order of the columns in clustering matters: it sorts by the first one first, then the second one, etc. If the first one has many distinct values, clustering by the second one becomes useless.\n",
    "\n",
    "* Clustering is useless if you also partition into small partitions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Group by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About group by, there is not much to say. No surprise here. You can for instance run:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  date,\n",
    "  SUM(new_confirmed) as total_new_confirmed\n",
    "FROM `myfirstname.covid19_bis`\n",
    "GROUP BY date\n",
    "```\n",
    "\n",
    "It will work as expected and will process exactly the same amount of data as:\n",
    "```sql\n",
    "SELECT\n",
    "  date,\n",
    "  new_confirmed\n",
    "FROM `myfirstname.covid19_bis`;\n",
    "```\n",
    "\n",
    "Of course, if you add a `WHERE` clause, then you can benefit from partitioning and clustering, the same as we saw before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Joins\n",
    "\n",
    "### 7.1. First attempt\n",
    "\n",
    "Let's create a `population` table with the population of each country per date. This would be useful to have this baseline to compute some statistics.\n",
    "\n",
    "```sql\n",
    "CREATE TABLE `myfirstname.population`\n",
    "PARTITION BY date\n",
    "CLUSTER BY country_name\n",
    "AS\n",
    "SELECT\n",
    "  date,\n",
    "  country_name,\n",
    "  population\n",
    "FROM `bigquery-public-data.covid19_open_data.covid19_open_data`;\n",
    "```\n",
    "\n",
    "And make the `JOIN`:\n",
    "```sql\n",
    "SELECT * FROM `myfirstname.covid19_third` c\n",
    "JOIN `myfirstname.population` p\n",
    "  ON p.country_name = c.country_name\n",
    "  AND p.date = c.date;\n",
    "```\n",
    "\n",
    "And... wait.\n",
    "\n",
    "Well, the query is very slow: more that 13 minutes and it is still running. If you are patient enough, you will eventually see it fail with this error: `too many results`.\n",
    "\n",
    "But wait: the number of result rows should be exactly the same as the number of rows in the original tables (since it is a one-to-one mapping)... What happened? Let's have a look at the _\"EXECUTION GRAPH\"_. It is important to look at it when a query takes too long or fails, to understand what is going on.\n",
    "\n",
    "<img src=\"join_exploding.png\" width=\"800\">\n",
    "\n",
    "You can see that there is an intermediate step with almost 50 billion rows that most probably is the culprit!\n",
    "\n",
    "If you want, you can try and play with partitioning and clustering on both tables but sadly, it will not help here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. How to make the join work?\n",
    "\n",
    "But wait, 22 million rows is quite a lot for the `population` table. If the data spans 3 years, it corresponds to 13 000 days. So it would mean there is ~1700 distinct countries. That is a little bit too much (there is around 200 countries in the world). So what happened?\n",
    "\n",
    "Let's inspect the data closer:\n",
    "```sql\n",
    "SELECT *\n",
    "FROM `bigquery-public-data.covid19_open_data.covid19_open_data`\n",
    "WHERE date='2021-09-06'\n",
    "AND country_name = 'Brazil';\n",
    "```\n",
    "\n",
    "There is ~5600 rows just for this one day this one country!\n",
    "\n",
    "This is because we forgot about the `location_key` column (which is e.g. `BR_CE_231140` or `BR_ES_320500` etc.)!\n",
    "\n",
    "We totally misunderstood the original dataset: one row is not one _country_ but one _location_ (maybe a city or district).\n",
    "\n",
    "So let's recreate our tables but this time we include the `location_key` column and join on it.\n",
    "\n",
    "NB: Of course we could ignore the location and do a `GROUP BY` country if we were interested only in the countries. But we keep the location for the demonstration, so that the dataset is not too small.\n",
    "\n",
    "```sql\n",
    "CREATE TABLE `myfirstname.covid19_loc`\n",
    "AS\n",
    "SELECT\n",
    "  date,\n",
    "  location_key,\n",
    "  country_name,\n",
    "  new_tested,\n",
    "  new_confirmed,\n",
    "  new_hospitalized_patients,\n",
    "  new_deceased\n",
    "FROM `bigquery-public-data.covid19_open_data.covid19_open_data`;\n",
    "```\n",
    "\n",
    "```sql\n",
    "CREATE TABLE `myfirstname.population_loc`\n",
    "AS\n",
    "SELECT\n",
    "  date,\n",
    "  country_name,\n",
    "  location_key,\n",
    "  population AS population\n",
    "FROM `bigquery-public-data.covid19_open_data.covid19_open_data`;\n",
    "```\n",
    "\n",
    "<img src=\"join_working_1.png\" width=\"750\">\n",
    "\n",
    "Boom! The query completes in 19 seconds! Finally ✌️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Debrief\n",
    "\n",
    "So what happened in the first case?\n",
    "\n",
    "Recall that one day and one country could have up to 5K rows and we joined on day and country only.\n",
    "\n",
    "So for each country and day, the `JOIN` creates 5K x 5K = 25 M rows! If we have 200 countries and 3 x 365 days, it is normal that the size of the final result is insane. \n",
    "\n",
    "> NB: With our mistake, we almost did a `CROSS JOIN` instead of an `INNER JOIN`.\n",
    "\n",
    "So the take-home message for joins is:\n",
    "\n",
    "---\n",
    "* Beware of duplicates.\n",
    "\n",
    "* Make sure you are joining on fields that are unique.\n",
    "\n",
    "* Look at the execution graph to see what is happening.\n",
    "---\n",
    "<br/>\n",
    "\n",
    "> A side note partitioning and clustering.\n",
    "> \n",
    "> For joins they have no impact on the cost, because we have to read all the data anyway and only reading data is charged.\n",
    "> \n",
    "> For large joins, they can help speed up a query though. In this case, there is no noticable impact (you can try multiple combinations and see by yourself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running BigQuery from Python\n",
    "\n",
    "To have a unified processing pipeline, we would like to run our queries from Python, same as we would do with regular SQL (e.g. MySQL).\n",
    "\n",
    "### 7.1. BigQuery setup in Python\n",
    "\n",
    "#### 7.1.1. Install `gcloud` Command-Line Interface (CLI)\n",
    "To set up the authentication, we need to first install `gcloud` CLI. It is a command that you will be able to run from the terminal to control GCP.\n",
    "\n",
    "Follow the instructions that correspond to your operating system. You will find them here: https://cloud.google.com/sdk/docs/install\n",
    "\n",
    "Don't forget to initialize `gcloud`:\n",
    "```bash\n",
    "gcloud init\n",
    "``` \n",
    "Then follow the instructions to define:\n",
    "* The default account you want to use (your personal Google account).\n",
    "* The default project you want to use: `da-bootcamp-2023`.\n",
    "\n",
    "#### 7.1.2. Set up Application Default Credentials (ADC)\n",
    "To authorize connection from Python, we first need to generate special \"application\" credentials. It is something like a big password that will allow you to make queries from Python as if you were connected to your Google account.\n",
    "\n",
    "You will find all the instructions to do so on this page: https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev\n",
    "\n",
    "But in short, you just need to type this command on your terminal:\n",
    "```bash\n",
    "gcloud auth application-default login\n",
    "```\n",
    "And then, in the browser page that will open, you need to authenticate with your Google credentials.\n",
    "\n",
    "You should see a message in the terminal that looks like:\n",
    "```\n",
    "Credentials saved to file: [/standard/config/path/application_default_credentials.json]\n",
    "```\n",
    "You can have a look at this file: it contains the credentials needed to connect to your Google Account with a command line. And it is also this file that the Python library will look at to authenticate you.\n",
    "\n",
    "#### 7.1.3. Install the Python BigQuery library\n",
    "As usual, simply run:\n",
    "```bash\n",
    "pip install google-cloud-bigquery\n",
    "# OR\n",
    "conda install google-cloud-bigquery\n",
    "```\n",
    "\n",
    "#### 7.1.4. Testing that everything works\n",
    "To test that everything went fine, you can run the following Python code. It should automatically authenticate you and print `hello` which is the results of this (free) test query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client(project=\"da-bootcamp-2023\")\n",
    "# For some reason, the library does not detect the default project previously set up with `gcloud init`\n",
    "# so you have to specify it manually each time.\n",
    "\n",
    "QUERY = \"SELECT 'hello' AS message\"\n",
    "query_job = client.query(QUERY)\n",
    "rows = query_job.result()\n",
    "\n",
    "for row in rows:\n",
    "    print(row.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. BigQuery setup with Pandas\n",
    "\n",
    "The first thing to note is that you can already (with the previous code) do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  message\n",
       "0   hello"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But BigQuery is even more nicely integrated with Pandas, with the function `pd.to_gbq` and `pd.read_gbq` which work similarly to `pd.to_sql` and `pd.read_sql`.\n",
    "\n",
    "To use them, you need to install `pandas-gbq`:\n",
    "\n",
    "```bash\n",
    "pip install pandas-gbq\n",
    "# OR\n",
    "conda install pandas-gbq\n",
    "```\n",
    "Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  message\n",
       "0   hello"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_gbq(\"SELECT 'hello' AS message\", project_id=\"da-bootcamp-2023\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Replace \"myfirstname\" by your first name\n",
    "df.to_gbq(\"myfirstname.hello\", project_id=\"da-bootcamp-2023\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  message\n",
       "0   hello"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_gbq(\"myfirstname.hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Running real queries from Python\n",
    "\n",
    "Now it is time to run an actual query from Python!\n",
    "\n",
    "But first, let's do a dry-run: it gives you the same (free of charge) cost estimate of your query that you would get from the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This query will process 0 bytes.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY = \"SELECT COUNT(*) FROM `myfirstname.covid19_third`\"\n",
    "\n",
    "query_job = client.query(QUERY, job_config=bigquery.QueryJobConfig(dry_run=True))\n",
    "f\"This query will process {query_job.total_bytes_processed} bytes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22756333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        f0_\n",
       "0  22756333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_gbq(QUERY, project_id=\"da-bootcamp-2023\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Executing other operations\n",
    "\n",
    "Actually, you can also use the Python library to execute any action that you would execute through the console. It may be handy to make your code more reproducable and shareable (execute Python code instead of clicking on a Web interface).\n",
    "\n",
    "For instance, you can automate the creation of a new dataset if it does not already exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists: nothing to do.\n"
     ]
    }
   ],
   "source": [
    "from google.api_core.exceptions import NotFound\n",
    "\n",
    "DATASET = \"myfirstname\"\n",
    "try:\n",
    "    client.get_dataset(DATASET)\n",
    "    print(\"Dataset already exists: nothing to do.\")\n",
    "except NotFound:\n",
    "    client.create_dataset(DATASET)\n",
    "    print(\"Dataset created\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also delete a table if you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_table(\"myfirstname.hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quotas\n",
    "\n",
    "There are two types of quotas in BigQuery:\n",
    "\n",
    "### 8.1. System quotas\n",
    "These are imposed by Google. You cannot change them except if you ask Google nicely. Some of them you cannot change because they are hard technical limits.\n",
    "\n",
    "There are _a lot_ of them. One example of system quotas is the maximum of 4 000 partitions per table that we already spoke about. But there are many more.\n",
    "\n",
    "Here is the full list: https://cloud.google.com/bigquery/quotas\n",
    "\n",
    "### 8.2. Organization quotas\n",
    "These are imposed by your account administrator, i.e. us, to make sure you do not run weird stuff that would incur crazy charges. \n",
    "\n",
    "You can see on [the _\"Quotas\"_ page](https://console.cloud.google.com/iam-admin/quotas?authuser=2&project=da-bootcamp-2023&pageState=(%22allQuotasTable%22:(%22s%22:%5B(%22i%22:%22serviceTitle%22,%22s%22:%220%22),(%22i%22:%22currentPercent%22,%22s%22:%221%22),(%22i%22:%22sevenDayPeakPercent%22,%22s%22:%220%22),(%22i%22:%22currentUsage%22,%22s%22:%221%22),(%22i%22:%22sevenDayPeakUsage%22,%22s%22:%220%22),(%22i%22:%22displayName%22,%22s%22:%220%22),(%22i%22:%22displayDimensions%22,%22s%22:%220%22)%5D,%22f%22:%22%255B%257B_22k_22_3A_22_22_2C_22t_22_3A10_2C_22v_22_3A_22_5C_22Query%2520usage_5C_22_22%257D%255D%22,%22r%22:200))) what your quota is and how much you already used:\n",
    "\n",
    "<img src=\"quotas.png\" width=\"950\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Take-home message: the main differences between MySQL et GoogleSQL\n",
    "\n",
    "There is a _lot_ more to say about BigQuery but we let you practice on your own, now that you know pretty much all the essentials.\n",
    "\n",
    "If you have questions, the best place to look at is BigQuery official documentation: https://cloud.google.com/bigquery/docs/\n",
    "\n",
    "Here is what you should remember:\n",
    "\n",
    "#### **There is no indexes and no foreign keys**\n",
    "Nothing is indexed in BigQuery. Your only hope is to use partitioning and clustering to reduce time and cost.\n",
    "\n",
    "#### **Joins are tricky and can be slow**\n",
    "This is a consequence of the previous point and of the fact that the data can be spread across multiple locations.\n",
    "\n",
    "Always make sure you are joining on fields that have **unique values**. If the query is too long, look at the **execution graph**.\n",
    "\n",
    "#### **All fields are nullable**\n",
    "In MySQL you specify for each field whether or not it can be null (which is important for indexes that cannot be nullable). In BigQuery, all fields can be null.\n",
    "\n",
    "#### **Time overhead**\n",
    "BigQuery is not good at responding fast to small simple queries. So small queries take longer than with a traditional relational database.\n",
    "\n",
    "Broadly speaking, it is not made for real-time production queries ([OLTP](https://en.wikipedia.org/wiki/Online_transaction_processing)) but for _a posteriori_ analytics query ([OLAP](https://en.wikipedia.org/wiki/Online_analytical_processing)).\n",
    "\n",
    "#### **Partitioning and clustering**\n",
    "These features usually don't exist in that form in relational databases like MySQL. You should understand how they work (see above).\n",
    "\n",
    "#### **Pay as you go**\n",
    "This is due to the _cloud_ nature of the service. You pay for _every_ query based on how much data has been _initially read_ and _finally written_ by your query.\n",
    "\n",
    "> The current costs are $6.25 per TiB (the first TiB per month per account is free)\n",
    ">\n",
    "> More on this: https://cloud.google.com/bigquery/pricing#analysis_pricing_models\n",
    "\n",
    "Note that for now (but this may change in the future):\n",
    "\n",
    "* BigQuery does not charge based on query time.\n",
    "* It does not charge intermediate reads and writes that it makes under the hood (only data that you explicitly read or write).\n",
    "* If you run a query multiple times in a row, only the first time gets charged, thanks to a _caching_ mechanism (data is stored in some cheaper-to-access storage).\n",
    "\n",
    "\n",
    "So you need to think twice before running a query, and especially look at the estimated execution costs.\n",
    "\n",
    "#### **Closed-source engine**\n",
    "BigQuery is a proprietary technology. In some way, it is similar to the open-source engine [Apache Cassandra](https://en.wikipedia.org/wiki/Apache_Cassandra) (which is also columnar and has similar partitioning and clustering logic). But we don't know exactly what the engine behind BigQuery is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
